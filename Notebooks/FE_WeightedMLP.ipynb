{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FE_WeightedMLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# mounting the google drive where the data csv files downloaded\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLByKRIRck9Y",
        "outputId": "2b6b23dd-789b-4dee-d2ea-fd81cd751884"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import tempfile\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "wsq23fYPcppL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/gdrive/My Drive/AIML/Project/train_combined.csv\")\n",
        "test_data = pd.read_csv(\"/gdrive/My Drive/AIML/Project/test_combined.csv\")\n",
        "neg, pos = np.bincount(train_data['isFraud'])"
      ],
      "metadata": {
        "id": "_wQwq5MrTqTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450b0199-726e-4da6-9eb9-a34b6c3e194b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (406,409,410,417,421,422,423,424,425,427,428,429,430,431,432,433,434) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (405,408,409,416,420,421,422,423,424,426,427,428,429,430,431,432,433) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "red_col = ['V1', 'V104', 'V107', 'V108', 'V11', 'V111', 'V115', 'V117', 'V120', 'V121', 'V123', 'V124', 'V127', 'V129', 'V13', 'V130', 'V136', 'V138', 'V139', 'V14', 'V142', 'V147', 'V156', 'V160', 'V162', 'V165', 'V166', 'V169', 'V17', 'V171', 'V173', 'V175', 'V176', 'V178', 'V180', 'V182', 'V185', 'V187', 'V188', 'V198', 'V20', 'V203', 'V205', 'V207', 'V209', 'V210', 'V215', 'V218', 'V220', 'V221', 'V223', 'V224', 'V226', 'V228', 'V229', 'V23', 'V234', 'V235', 'V238', 'V240', 'V250', 'V252', 'V253', 'V257', 'V258', 'V26', 'V260', 'V261', 'V264', 'V266', 'V267', 'V27', 'V271', 'V274', 'V277', 'V281', 'V283', 'V284', 'V285', 'V286', 'V289', 'V291', 'V294', 'V296', 'V297', 'V3', 'V30', 'V301', 'V303', 'V305', 'V307', 'V309', 'V310', 'V314', 'V320', 'V325', 'V332', 'V335', 'V338', 'V36', 'V37', 'V4', 'V40', 'V41', 'V44', 'V47', 'V48', 'V54', 'V55', 'V56', 'V59', 'V6', 'V62', 'V65', 'V67', 'V68', 'V70', 'V76', 'V78', 'V8', 'V80', 'V82', 'V86', 'V88', 'V89', 'V91', 'V96', 'V98', 'V99']\n",
        "# droping v cols \n",
        "drop_cols = [col for col in train_data.columns if col[0] == 'V' and col not in red_col]"
      ],
      "metadata": {
        "id": "viP9yF8VRo2Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'dropping {len(drop_cols)} columns')\n",
        "train_data = train_data.drop(columns=drop_cols)\n",
        "test_data = test_data.drop(columns=drop_cols)"
      ],
      "metadata": {
        "id": "7whuqQbwRxWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "413d2bfe-ff1d-4517-8f31-78c0172cc39d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dropping 210 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "useful_cols_tr = [col for col in train_data.columns if col not in (\"TransactionID_x\", \"TransactionID_y\", \"TransactionDT\")]\n",
        "print(len(useful_cols_tr))\n",
        "\n",
        "useful_cols = [col for col in train_data.columns if col not in (\"isFraud\",\"TransactionID_x\", \"TransactionID_y\", \"TransactionDT\")]\n",
        "print(len(useful_cols))\n",
        "\n",
        "cols = [col for col in train_data]\n",
        "cols.remove(\"isFraud\")\n",
        "test_data.columns = cols\n",
        "x_test = test_data[useful_cols].reset_index(drop=True)\n",
        "x_train = train_data[useful_cols_tr].reset_index(drop=True)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "ExljphZ9nZvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e8d889-c80e-4768-bf3c-5851e7334337"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "222\n",
            "221\n",
            "(590540, 222)\n",
            "(506691, 221)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_cols = x_train.select_dtypes(include=['object']).columns\n",
        "type_map = {c: str for c in category_cols}\n",
        "x_train[category_cols] = x_train[category_cols].astype(type_map, copy=False)\n",
        "x_test[category_cols] = x_test[category_cols].astype(type_map, copy=False)\n",
        "\n",
        "for col in category_cols:\n",
        "    # label encode all cat columns\n",
        "    dff = pd.concat([x_train[col],x_test[col]])\n",
        "    dff,_ = pd.factorize(dff,sort=True)\n",
        "    if dff.max()>32000: \n",
        "        print(col,'needs int32 datatype')\n",
        "           \n",
        "    x_train[col] = dff[:len(x_train)].astype('int16')\n",
        "    x_test[col] = dff[len(x_train):].astype('int16')\n",
        "del dff"
      ],
      "metadata": {
        "id": "ScM5vXNLpwrS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The `Amount` column covers a huge range. Convert to log-space.\n",
        "eps = 0.001 # 0 => 0.1Â¢\n",
        "x_train['Log Ammount'] = np.log(x_train.pop('TransactionAmt')+eps)\n",
        "x_test['Log Ammount'] = np.log(x_test.pop('TransactionAmt')+eps)\n",
        "\n",
        "# Scaling numeric features\n",
        "for col in useful_cols:\n",
        "    if col not in category_cols and col not in ['TransactionAmt','isFraud']:\n",
        "        # min max scalar\n",
        "        dff = pd.concat([x_train[col],x_test[col]])\n",
        "        dff = (dff - dff.min())/(dff.max() - dff.min())\n",
        "        dff.fillna(-1,inplace=True)\n",
        "\n",
        "        x_train[col] = dff[:len(x_train)]\n",
        "        x_test[col] = dff[len(x_train):]\n",
        "\n",
        "del dff\n",
        "\n",
        "print(f'fitting model on {len(useful_cols)} columns')\n",
        "x_train.fillna(-1,inplace=True)\n",
        "x_test.fillna(-1,inplace=True)"
      ],
      "metadata": {
        "id": "4K000SIHVu_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9449d04-b5ff-4fe9-f343-cabb3e79560c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitting model on 221 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))\n",
        "\n",
        "# Use a utility from sklearn to split and shuffle your dataset.\n",
        "train_df, val_df = train_test_split(x_train, test_size=0.2)\n",
        "\n",
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n"
      ],
      "metadata": {
        "id": "2QZwZLNJonWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22b8879-1957-41a8-e620-d8de1e4032a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples:\n",
            "    Total: 590540\n",
            "    Positive: 20663 (3.50% of total)\n",
            "\n",
            "Weight for class 0: 0.52\n",
            "Weight for class 1: 14.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Form np arrays of labels and features.\n",
        "train_labels = np.array(train_df.pop(\"isFraud\"))\n",
        "bool_train_labels = train_labels != 0\n",
        "val_labels = np.array(val_df.pop(\"isFraud\"))\n",
        "test_labels = np.array(x_test)\n",
        "\n",
        "train_features = np.array(train_df)\n",
        "val_features = np.array(val_df)\n",
        "test_features = np.array(x_test)\n",
        "\n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Validation features shape:', val_features.shape)\n",
        "print('Test features shape:', test_features.shape)\n",
        "\n",
        "train_features = np.clip(train_features, -5, 5)\n",
        "val_features = np.clip(val_features, -5, 5)\n",
        "test_features = np.clip(test_features, -5, 5)\n",
        "\n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Validation labels shape:', val_labels.shape)\n",
        "print('Test labels shape:', test_labels.shape)"
      ],
      "metadata": {
        "id": "yrXsIxylo6GX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4354402a-9706-4f38-fadd-1638bf814e5e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training features shape: (472432, 221)\n",
            "Validation features shape: (118108, 221)\n",
            "Test features shape: (506691, 221)\n",
            "Training labels shape: (472432,)\n",
            "Validation labels shape: (118108,)\n",
            "Test labels shape: (506691, 221)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc', curve=\"ROC\"),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "def make_model(metrics=METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          256, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(\n",
        "          128, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(\n",
        "          64, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "            keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1, activation='sigmoid',\n",
        "          bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "H8OvDA7opueI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200\n",
        "BATCH_SIZE = 4096\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_prc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "weighted_model = make_model()\n",
        "#weighted_model.load_weights(initial_weights)\n",
        "weighted_model.summary()\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight)\n"
      ],
      "metadata": {
        "id": "imI57ROJpz91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7976a8e-bc63-453c-a086-588daeba4d5d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               56832     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 99,041\n",
            "Trainable params: 99,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "116/116 [==============================] - 15s 98ms/step - loss: 0.6873 - tp: 11814.0000 - fp: 254130.0000 - tn: 201828.0000 - fn: 4660.0000 - accuracy: 0.4522 - precision: 0.0444 - recall: 0.7171 - auc: 0.6195 - prc: 0.0543 - val_loss: 0.7339 - val_tp: 3670.0000 - val_fp: 61543.0000 - val_tn: 52376.0000 - val_fn: 519.0000 - val_accuracy: 0.4745 - val_precision: 0.0563 - val_recall: 0.8761 - val_auc: 0.7682 - val_prc: 0.1734\n",
            "Epoch 2/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.6138 - tp: 12109.0000 - fp: 192279.0000 - tn: 263679.0000 - fn: 4365.0000 - accuracy: 0.5838 - precision: 0.0592 - recall: 0.7350 - auc: 0.7294 - prc: 0.1073 - val_loss: 0.5726 - val_tp: 3224.0000 - val_fp: 37594.0000 - val_tn: 76325.0000 - val_fn: 965.0000 - val_accuracy: 0.6735 - val_precision: 0.0790 - val_recall: 0.7696 - val_auc: 0.7965 - val_prc: 0.2127\n",
            "Epoch 3/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5874 - tp: 11891.0000 - fp: 155686.0000 - tn: 300272.0000 - fn: 4583.0000 - accuracy: 0.6608 - precision: 0.0710 - recall: 0.7218 - auc: 0.7622 - prc: 0.1427 - val_loss: 0.6076 - val_tp: 3198.0000 - val_fp: 32862.0000 - val_tn: 81057.0000 - val_fn: 991.0000 - val_accuracy: 0.7134 - val_precision: 0.0887 - val_recall: 0.7634 - val_auc: 0.8109 - val_prc: 0.2381\n",
            "Epoch 4/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5712 - tp: 11653.0000 - fp: 134887.0000 - tn: 321071.0000 - fn: 4821.0000 - accuracy: 0.7043 - precision: 0.0795 - recall: 0.7074 - auc: 0.7778 - prc: 0.1682 - val_loss: 0.5629 - val_tp: 3002.0000 - val_fp: 26188.0000 - val_tn: 87731.0000 - val_fn: 1187.0000 - val_accuracy: 0.7682 - val_precision: 0.1028 - val_recall: 0.7166 - val_auc: 0.8179 - val_prc: 0.2541\n",
            "Epoch 5/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5604 - tp: 11511.0000 - fp: 120432.0000 - tn: 335526.0000 - fn: 4963.0000 - accuracy: 0.7346 - precision: 0.0872 - recall: 0.6987 - auc: 0.7865 - prc: 0.1890 - val_loss: 0.4970 - val_tp: 2782.0000 - val_fp: 19515.0000 - val_tn: 94404.0000 - val_fn: 1407.0000 - val_accuracy: 0.8229 - val_precision: 0.1248 - val_recall: 0.6641 - val_auc: 0.8263 - val_prc: 0.2706\n",
            "Epoch 6/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.5529 - tp: 11471.0000 - fp: 111653.0000 - tn: 344305.0000 - fn: 5003.0000 - accuracy: 0.7531 - precision: 0.0932 - recall: 0.6963 - auc: 0.7937 - prc: 0.2024 - val_loss: 0.5003 - val_tp: 2918.0000 - val_fp: 21553.0000 - val_tn: 92366.0000 - val_fn: 1271.0000 - val_accuracy: 0.8068 - val_precision: 0.1192 - val_recall: 0.6966 - val_auc: 0.8323 - val_prc: 0.2828\n",
            "Epoch 7/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.5487 - tp: 11386.0000 - fp: 105917.0000 - tn: 350041.0000 - fn: 5088.0000 - accuracy: 0.7650 - precision: 0.0971 - recall: 0.6911 - auc: 0.7969 - prc: 0.2053 - val_loss: 0.5389 - val_tp: 3041.0000 - val_fp: 24954.0000 - val_tn: 88965.0000 - val_fn: 1148.0000 - val_accuracy: 0.7790 - val_precision: 0.1086 - val_recall: 0.7259 - val_auc: 0.8298 - val_prc: 0.2841\n",
            "Epoch 8/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.5461 - tp: 11495.0000 - fp: 106048.0000 - tn: 349910.0000 - fn: 4979.0000 - accuracy: 0.7650 - precision: 0.0978 - recall: 0.6978 - auc: 0.8005 - prc: 0.2103 - val_loss: 0.5454 - val_tp: 3172.0000 - val_fp: 28364.0000 - val_tn: 85555.0000 - val_fn: 1017.0000 - val_accuracy: 0.7512 - val_precision: 0.1006 - val_recall: 0.7572 - val_auc: 0.8355 - val_prc: 0.2945\n",
            "Epoch 9/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.5399 - tp: 11604.0000 - fp: 104408.0000 - tn: 351550.0000 - fn: 4870.0000 - accuracy: 0.7687 - precision: 0.1000 - recall: 0.7044 - auc: 0.8051 - prc: 0.2162 - val_loss: 0.5403 - val_tp: 3140.0000 - val_fp: 26046.0000 - val_tn: 87873.0000 - val_fn: 1049.0000 - val_accuracy: 0.7706 - val_precision: 0.1076 - val_recall: 0.7496 - val_auc: 0.8368 - val_prc: 0.3006\n",
            "Epoch 10/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5376 - tp: 11479.0000 - fp: 100821.0000 - tn: 355137.0000 - fn: 4995.0000 - accuracy: 0.7760 - precision: 0.1022 - recall: 0.6968 - auc: 0.8063 - prc: 0.2239 - val_loss: 0.4967 - val_tp: 2899.0000 - val_fp: 20061.0000 - val_tn: 93858.0000 - val_fn: 1290.0000 - val_accuracy: 0.8192 - val_precision: 0.1263 - val_recall: 0.6921 - val_auc: 0.8383 - val_prc: 0.3108\n",
            "Epoch 11/200\n",
            "116/116 [==============================] - 15s 128ms/step - loss: 0.5358 - tp: 11378.0000 - fp: 98377.0000 - tn: 357581.0000 - fn: 5096.0000 - accuracy: 0.7810 - precision: 0.1037 - recall: 0.6907 - auc: 0.8079 - prc: 0.2272 - val_loss: 0.4835 - val_tp: 2904.0000 - val_fp: 20200.0000 - val_tn: 93719.0000 - val_fn: 1285.0000 - val_accuracy: 0.8181 - val_precision: 0.1257 - val_recall: 0.6932 - val_auc: 0.8399 - val_prc: 0.3117\n",
            "Epoch 12/200\n",
            "116/116 [==============================] - 17s 146ms/step - loss: 0.5328 - tp: 11367.0000 - fp: 95713.0000 - tn: 360245.0000 - fn: 5107.0000 - accuracy: 0.7866 - precision: 0.1062 - recall: 0.6900 - auc: 0.8103 - prc: 0.2319 - val_loss: 0.4850 - val_tp: 2734.0000 - val_fp: 16957.0000 - val_tn: 96962.0000 - val_fn: 1455.0000 - val_accuracy: 0.8441 - val_precision: 0.1388 - val_recall: 0.6527 - val_auc: 0.8395 - val_prc: 0.3188\n",
            "Epoch 13/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5332 - tp: 11365.0000 - fp: 97515.0000 - tn: 358443.0000 - fn: 5109.0000 - accuracy: 0.7828 - precision: 0.1044 - recall: 0.6899 - auc: 0.8098 - prc: 0.2405 - val_loss: 0.5308 - val_tp: 3153.0000 - val_fp: 27007.0000 - val_tn: 86912.0000 - val_fn: 1036.0000 - val_accuracy: 0.7626 - val_precision: 0.1045 - val_recall: 0.7527 - val_auc: 0.8384 - val_prc: 0.3227\n",
            "Epoch 14/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5276 - tp: 11661.0000 - fp: 101051.0000 - tn: 354907.0000 - fn: 4813.0000 - accuracy: 0.7759 - precision: 0.1035 - recall: 0.7078 - auc: 0.8139 - prc: 0.2466 - val_loss: 0.5559 - val_tp: 3173.0000 - val_fp: 26005.0000 - val_tn: 87914.0000 - val_fn: 1016.0000 - val_accuracy: 0.7712 - val_precision: 0.1087 - val_recall: 0.7575 - val_auc: 0.8440 - val_prc: 0.3279\n",
            "Epoch 15/200\n",
            "116/116 [==============================] - 9s 82ms/step - loss: 0.5301 - tp: 11406.0000 - fp: 95972.0000 - tn: 359986.0000 - fn: 5068.0000 - accuracy: 0.7861 - precision: 0.1062 - recall: 0.6924 - auc: 0.8127 - prc: 0.2414 - val_loss: 0.5224 - val_tp: 3155.0000 - val_fp: 25032.0000 - val_tn: 88887.0000 - val_fn: 1034.0000 - val_accuracy: 0.7793 - val_precision: 0.1119 - val_recall: 0.7532 - val_auc: 0.8460 - val_prc: 0.3273\n",
            "Epoch 16/200\n",
            "116/116 [==============================] - 9s 81ms/step - loss: 0.5276 - tp: 11561.0000 - fp: 98275.0000 - tn: 357683.0000 - fn: 4913.0000 - accuracy: 0.7816 - precision: 0.1053 - recall: 0.7018 - auc: 0.8148 - prc: 0.2460 - val_loss: 0.5067 - val_tp: 3085.0000 - val_fp: 23554.0000 - val_tn: 90365.0000 - val_fn: 1104.0000 - val_accuracy: 0.7912 - val_precision: 0.1158 - val_recall: 0.7365 - val_auc: 0.8431 - val_prc: 0.3251\n",
            "Epoch 17/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.5280 - tp: 11441.0000 - fp: 94113.0000 - tn: 361845.0000 - fn: 5033.0000 - accuracy: 0.7901 - precision: 0.1084 - recall: 0.6945 - auc: 0.8137 - prc: 0.2447 - val_loss: 0.5235 - val_tp: 3136.0000 - val_fp: 23647.0000 - val_tn: 90272.0000 - val_fn: 1053.0000 - val_accuracy: 0.7909 - val_precision: 0.1171 - val_recall: 0.7486 - val_auc: 0.8458 - val_prc: 0.3293\n",
            "Epoch 18/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.5291 - tp: 11464.0000 - fp: 96635.0000 - tn: 359323.0000 - fn: 5010.0000 - accuracy: 0.7848 - precision: 0.1061 - recall: 0.6959 - auc: 0.8132 - prc: 0.2464 - val_loss: 0.4875 - val_tp: 3057.0000 - val_fp: 21454.0000 - val_tn: 92465.0000 - val_fn: 1132.0000 - val_accuracy: 0.8088 - val_precision: 0.1247 - val_recall: 0.7298 - val_auc: 0.8477 - val_prc: 0.3445\n",
            "Epoch 19/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5216 - tp: 11276.0000 - fp: 87491.0000 - tn: 368467.0000 - fn: 5198.0000 - accuracy: 0.8038 - precision: 0.1142 - recall: 0.6845 - auc: 0.8178 - prc: 0.2644 - val_loss: 0.4584 - val_tp: 2895.0000 - val_fp: 18000.0000 - val_tn: 95919.0000 - val_fn: 1294.0000 - val_accuracy: 0.8366 - val_precision: 0.1385 - val_recall: 0.6911 - val_auc: 0.8486 - val_prc: 0.3474\n",
            "Epoch 20/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.5208 - tp: 11422.0000 - fp: 91337.0000 - tn: 364621.0000 - fn: 5052.0000 - accuracy: 0.7960 - precision: 0.1112 - recall: 0.6933 - auc: 0.8192 - prc: 0.2639 - val_loss: 0.4715 - val_tp: 3078.0000 - val_fp: 21768.0000 - val_tn: 92151.0000 - val_fn: 1111.0000 - val_accuracy: 0.8063 - val_precision: 0.1239 - val_recall: 0.7348 - val_auc: 0.8495 - val_prc: 0.3512\n",
            "Epoch 21/200\n",
            "116/116 [==============================] - 11s 95ms/step - loss: 0.5214 - tp: 11338.0000 - fp: 87254.0000 - tn: 368704.0000 - fn: 5136.0000 - accuracy: 0.8044 - precision: 0.1150 - recall: 0.6882 - auc: 0.8177 - prc: 0.2646 - val_loss: 0.4870 - val_tp: 3023.0000 - val_fp: 20204.0000 - val_tn: 93715.0000 - val_fn: 1166.0000 - val_accuracy: 0.8191 - val_precision: 0.1302 - val_recall: 0.7217 - val_auc: 0.8496 - val_prc: 0.3535\n",
            "Epoch 22/200\n",
            "116/116 [==============================] - 11s 94ms/step - loss: 0.5223 - tp: 11158.0000 - fp: 85034.0000 - tn: 370924.0000 - fn: 5316.0000 - accuracy: 0.8088 - precision: 0.1160 - recall: 0.6773 - auc: 0.8174 - prc: 0.2692 - val_loss: 0.5044 - val_tp: 3077.0000 - val_fp: 22059.0000 - val_tn: 91860.0000 - val_fn: 1112.0000 - val_accuracy: 0.8038 - val_precision: 0.1224 - val_recall: 0.7345 - val_auc: 0.8506 - val_prc: 0.3579\n",
            "Epoch 23/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.5190 - tp: 11275.0000 - fp: 84937.0000 - tn: 371021.0000 - fn: 5199.0000 - accuracy: 0.8092 - precision: 0.1172 - recall: 0.6844 - auc: 0.8201 - prc: 0.2693 - val_loss: 0.5166 - val_tp: 3139.0000 - val_fp: 24364.0000 - val_tn: 89555.0000 - val_fn: 1050.0000 - val_accuracy: 0.7848 - val_precision: 0.1141 - val_recall: 0.7493 - val_auc: 0.8495 - val_prc: 0.3561\n",
            "Epoch 24/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5182 - tp: 11302.0000 - fp: 86874.0000 - tn: 369084.0000 - fn: 5172.0000 - accuracy: 0.8052 - precision: 0.1151 - recall: 0.6861 - auc: 0.8208 - prc: 0.2703 - val_loss: 0.4910 - val_tp: 2955.0000 - val_fp: 18121.0000 - val_tn: 95798.0000 - val_fn: 1234.0000 - val_accuracy: 0.8361 - val_precision: 0.1402 - val_recall: 0.7054 - val_auc: 0.8518 - val_prc: 0.3607\n",
            "Epoch 25/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5144 - tp: 11244.0000 - fp: 84068.0000 - tn: 371890.0000 - fn: 5230.0000 - accuracy: 0.8110 - precision: 0.1180 - recall: 0.6825 - auc: 0.8243 - prc: 0.2804 - val_loss: 0.5163 - val_tp: 3055.0000 - val_fp: 21048.0000 - val_tn: 92871.0000 - val_fn: 1134.0000 - val_accuracy: 0.8122 - val_precision: 0.1267 - val_recall: 0.7293 - val_auc: 0.8537 - val_prc: 0.3646\n",
            "Epoch 26/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5145 - tp: 11269.0000 - fp: 82856.0000 - tn: 373102.0000 - fn: 5205.0000 - accuracy: 0.8136 - precision: 0.1197 - recall: 0.6840 - auc: 0.8238 - prc: 0.2830 - val_loss: 0.4842 - val_tp: 2900.0000 - val_fp: 17229.0000 - val_tn: 96690.0000 - val_fn: 1289.0000 - val_accuracy: 0.8432 - val_precision: 0.1441 - val_recall: 0.6923 - val_auc: 0.8489 - val_prc: 0.3618\n",
            "Epoch 27/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5181 - tp: 11230.0000 - fp: 84843.0000 - tn: 371115.0000 - fn: 5244.0000 - accuracy: 0.8093 - precision: 0.1169 - recall: 0.6817 - auc: 0.8213 - prc: 0.2783 - val_loss: 0.5014 - val_tp: 3067.0000 - val_fp: 20613.0000 - val_tn: 93306.0000 - val_fn: 1122.0000 - val_accuracy: 0.8160 - val_precision: 0.1295 - val_recall: 0.7322 - val_auc: 0.8530 - val_prc: 0.3694\n",
            "Epoch 28/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5131 - tp: 11334.0000 - fp: 84194.0000 - tn: 371764.0000 - fn: 5140.0000 - accuracy: 0.8109 - precision: 0.1186 - recall: 0.6880 - auc: 0.8247 - prc: 0.2849 - val_loss: 0.5323 - val_tp: 3107.0000 - val_fp: 21800.0000 - val_tn: 92119.0000 - val_fn: 1082.0000 - val_accuracy: 0.8063 - val_precision: 0.1247 - val_recall: 0.7417 - val_auc: 0.8505 - val_prc: 0.3631\n",
            "Epoch 29/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5141 - tp: 11336.0000 - fp: 86618.0000 - tn: 369340.0000 - fn: 5138.0000 - accuracy: 0.8058 - precision: 0.1157 - recall: 0.6881 - auc: 0.8249 - prc: 0.2777 - val_loss: 0.4709 - val_tp: 2877.0000 - val_fp: 17198.0000 - val_tn: 96721.0000 - val_fn: 1312.0000 - val_accuracy: 0.8433 - val_precision: 0.1433 - val_recall: 0.6868 - val_auc: 0.8503 - val_prc: 0.3581\n",
            "Epoch 30/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5133 - tp: 11437.0000 - fp: 88758.0000 - tn: 367200.0000 - fn: 5037.0000 - accuracy: 0.8015 - precision: 0.1141 - recall: 0.6942 - auc: 0.8249 - prc: 0.2839 - val_loss: 0.4995 - val_tp: 2952.0000 - val_fp: 18992.0000 - val_tn: 94927.0000 - val_fn: 1237.0000 - val_accuracy: 0.8287 - val_precision: 0.1345 - val_recall: 0.7047 - val_auc: 0.8517 - val_prc: 0.3664\n",
            "Epoch 31/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5146 - tp: 11338.0000 - fp: 85981.0000 - tn: 369977.0000 - fn: 5136.0000 - accuracy: 0.8071 - precision: 0.1165 - recall: 0.6882 - auc: 0.8238 - prc: 0.2858 - val_loss: 0.4657 - val_tp: 2851.0000 - val_fp: 16901.0000 - val_tn: 97018.0000 - val_fn: 1338.0000 - val_accuracy: 0.8456 - val_precision: 0.1443 - val_recall: 0.6806 - val_auc: 0.8534 - val_prc: 0.3706\n",
            "Epoch 32/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5121 - tp: 11363.0000 - fp: 84529.0000 - tn: 371429.0000 - fn: 5111.0000 - accuracy: 0.8103 - precision: 0.1185 - recall: 0.6898 - auc: 0.8260 - prc: 0.2862 - val_loss: 0.4776 - val_tp: 2819.0000 - val_fp: 15358.0000 - val_tn: 98561.0000 - val_fn: 1370.0000 - val_accuracy: 0.8584 - val_precision: 0.1551 - val_recall: 0.6730 - val_auc: 0.8552 - val_prc: 0.3748\n",
            "Epoch 33/200\n",
            "116/116 [==============================] - 12s 104ms/step - loss: 0.5142 - tp: 11193.0000 - fp: 80693.0000 - tn: 375265.0000 - fn: 5281.0000 - accuracy: 0.8180 - precision: 0.1218 - recall: 0.6794 - auc: 0.8239 - prc: 0.2854 - val_loss: 0.4919 - val_tp: 3159.0000 - val_fp: 23178.0000 - val_tn: 90741.0000 - val_fn: 1030.0000 - val_accuracy: 0.7950 - val_precision: 0.1199 - val_recall: 0.7541 - val_auc: 0.8562 - val_prc: 0.3834\n",
            "Epoch 34/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5127 - tp: 11249.0000 - fp: 82275.0000 - tn: 373683.0000 - fn: 5225.0000 - accuracy: 0.8148 - precision: 0.1203 - recall: 0.6828 - auc: 0.8251 - prc: 0.2900 - val_loss: 0.4965 - val_tp: 3077.0000 - val_fp: 20806.0000 - val_tn: 93113.0000 - val_fn: 1112.0000 - val_accuracy: 0.8144 - val_precision: 0.1288 - val_recall: 0.7345 - val_auc: 0.8559 - val_prc: 0.3786\n",
            "Epoch 35/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5111 - tp: 11345.0000 - fp: 85292.0000 - tn: 370666.0000 - fn: 5129.0000 - accuracy: 0.8086 - precision: 0.1174 - recall: 0.6887 - auc: 0.8264 - prc: 0.2927 - val_loss: 0.5009 - val_tp: 2958.0000 - val_fp: 19798.0000 - val_tn: 94121.0000 - val_fn: 1231.0000 - val_accuracy: 0.8220 - val_precision: 0.1300 - val_recall: 0.7061 - val_auc: 0.8488 - val_prc: 0.3675\n",
            "Epoch 36/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5087 - tp: 11242.0000 - fp: 79842.0000 - tn: 376116.0000 - fn: 5232.0000 - accuracy: 0.8199 - precision: 0.1234 - recall: 0.6824 - auc: 0.8278 - prc: 0.2946 - val_loss: 0.4712 - val_tp: 2883.0000 - val_fp: 16510.0000 - val_tn: 97409.0000 - val_fn: 1306.0000 - val_accuracy: 0.8492 - val_precision: 0.1487 - val_recall: 0.6882 - val_auc: 0.8556 - val_prc: 0.3807\n",
            "Epoch 37/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5082 - tp: 11337.0000 - fp: 83341.0000 - tn: 372617.0000 - fn: 5137.0000 - accuracy: 0.8127 - precision: 0.1197 - recall: 0.6882 - auc: 0.8279 - prc: 0.2983 - val_loss: 0.4985 - val_tp: 3056.0000 - val_fp: 19777.0000 - val_tn: 94142.0000 - val_fn: 1133.0000 - val_accuracy: 0.8230 - val_precision: 0.1338 - val_recall: 0.7295 - val_auc: 0.8578 - val_prc: 0.3880\n",
            "Epoch 38/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5112 - tp: 11188.0000 - fp: 80008.0000 - tn: 375950.0000 - fn: 5286.0000 - accuracy: 0.8195 - precision: 0.1227 - recall: 0.6791 - auc: 0.8255 - prc: 0.2993 - val_loss: 0.4564 - val_tp: 2952.0000 - val_fp: 17965.0000 - val_tn: 95954.0000 - val_fn: 1237.0000 - val_accuracy: 0.8374 - val_precision: 0.1411 - val_recall: 0.7047 - val_auc: 0.8568 - val_prc: 0.3824\n",
            "Epoch 39/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5070 - tp: 11312.0000 - fp: 79493.0000 - tn: 376465.0000 - fn: 5162.0000 - accuracy: 0.8208 - precision: 0.1246 - recall: 0.6867 - auc: 0.8303 - prc: 0.2983 - val_loss: 0.4969 - val_tp: 3112.0000 - val_fp: 20917.0000 - val_tn: 93002.0000 - val_fn: 1077.0000 - val_accuracy: 0.8138 - val_precision: 0.1295 - val_recall: 0.7429 - val_auc: 0.8579 - val_prc: 0.3893\n",
            "Epoch 40/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.5092 - tp: 11265.0000 - fp: 81812.0000 - tn: 374146.0000 - fn: 5209.0000 - accuracy: 0.8158 - precision: 0.1210 - recall: 0.6838 - auc: 0.8259 - prc: 0.3030 - val_loss: 0.4433 - val_tp: 2754.0000 - val_fp: 13514.0000 - val_tn: 100405.0000 - val_fn: 1435.0000 - val_accuracy: 0.8734 - val_precision: 0.1693 - val_recall: 0.6574 - val_auc: 0.8582 - val_prc: 0.3877\n",
            "Epoch 41/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5092 - tp: 11162.0000 - fp: 79192.0000 - tn: 376766.0000 - fn: 5312.0000 - accuracy: 0.8211 - precision: 0.1235 - recall: 0.6776 - auc: 0.8266 - prc: 0.3002 - val_loss: 0.4785 - val_tp: 2936.0000 - val_fp: 18173.0000 - val_tn: 95746.0000 - val_fn: 1253.0000 - val_accuracy: 0.8355 - val_precision: 0.1391 - val_recall: 0.7009 - val_auc: 0.8540 - val_prc: 0.3652\n",
            "Epoch 42/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5067 - tp: 11162.0000 - fp: 77399.0000 - tn: 378559.0000 - fn: 5312.0000 - accuracy: 0.8249 - precision: 0.1260 - recall: 0.6776 - auc: 0.8291 - prc: 0.3040 - val_loss: 0.5203 - val_tp: 3232.0000 - val_fp: 25048.0000 - val_tn: 88871.0000 - val_fn: 957.0000 - val_accuracy: 0.7798 - val_precision: 0.1143 - val_recall: 0.7715 - val_auc: 0.8572 - val_prc: 0.3835\n",
            "Epoch 43/200\n",
            "116/116 [==============================] - 10s 82ms/step - loss: 0.5036 - tp: 11261.0000 - fp: 78544.0000 - tn: 377414.0000 - fn: 5213.0000 - accuracy: 0.8227 - precision: 0.1254 - recall: 0.6836 - auc: 0.8318 - prc: 0.3091 - val_loss: 0.4620 - val_tp: 2941.0000 - val_fp: 17029.0000 - val_tn: 96890.0000 - val_fn: 1248.0000 - val_accuracy: 0.8453 - val_precision: 0.1473 - val_recall: 0.7021 - val_auc: 0.8595 - val_prc: 0.3962\n",
            "Epoch 44/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5048 - tp: 11271.0000 - fp: 77812.0000 - tn: 378146.0000 - fn: 5203.0000 - accuracy: 0.8243 - precision: 0.1265 - recall: 0.6842 - auc: 0.8297 - prc: 0.3125 - val_loss: 0.4924 - val_tp: 3085.0000 - val_fp: 19734.0000 - val_tn: 94185.0000 - val_fn: 1104.0000 - val_accuracy: 0.8236 - val_precision: 0.1352 - val_recall: 0.7365 - val_auc: 0.8595 - val_prc: 0.4008\n",
            "Epoch 45/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5053 - tp: 11176.0000 - fp: 78134.0000 - tn: 377824.0000 - fn: 5298.0000 - accuracy: 0.8234 - precision: 0.1251 - recall: 0.6784 - auc: 0.8292 - prc: 0.3055 - val_loss: 0.4609 - val_tp: 3028.0000 - val_fp: 18380.0000 - val_tn: 95539.0000 - val_fn: 1161.0000 - val_accuracy: 0.8345 - val_precision: 0.1414 - val_recall: 0.7228 - val_auc: 0.8596 - val_prc: 0.4012\n",
            "Epoch 46/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5049 - tp: 11227.0000 - fp: 75908.0000 - tn: 380050.0000 - fn: 5247.0000 - accuracy: 0.8282 - precision: 0.1288 - recall: 0.6815 - auc: 0.8299 - prc: 0.3046 - val_loss: 0.4379 - val_tp: 2800.0000 - val_fp: 14805.0000 - val_tn: 99114.0000 - val_fn: 1389.0000 - val_accuracy: 0.8629 - val_precision: 0.1590 - val_recall: 0.6684 - val_auc: 0.8584 - val_prc: 0.4020\n",
            "Epoch 47/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5064 - tp: 11162.0000 - fp: 77214.0000 - tn: 378744.0000 - fn: 5312.0000 - accuracy: 0.8253 - precision: 0.1263 - recall: 0.6776 - auc: 0.8275 - prc: 0.3074 - val_loss: 0.4895 - val_tp: 3109.0000 - val_fp: 21077.0000 - val_tn: 92842.0000 - val_fn: 1080.0000 - val_accuracy: 0.8124 - val_precision: 0.1285 - val_recall: 0.7422 - val_auc: 0.8575 - val_prc: 0.3858\n",
            "Epoch 48/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5031 - tp: 11279.0000 - fp: 80191.0000 - tn: 375767.0000 - fn: 5195.0000 - accuracy: 0.8193 - precision: 0.1233 - recall: 0.6847 - auc: 0.8315 - prc: 0.3127 - val_loss: 0.5077 - val_tp: 3175.0000 - val_fp: 22865.0000 - val_tn: 91054.0000 - val_fn: 1014.0000 - val_accuracy: 0.7978 - val_precision: 0.1219 - val_recall: 0.7579 - val_auc: 0.8571 - val_prc: 0.4039\n",
            "Epoch 49/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5054 - tp: 11059.0000 - fp: 75141.0000 - tn: 380817.0000 - fn: 5415.0000 - accuracy: 0.8295 - precision: 0.1283 - recall: 0.6713 - auc: 0.8288 - prc: 0.3119 - val_loss: 0.4479 - val_tp: 2909.0000 - val_fp: 15706.0000 - val_tn: 98213.0000 - val_fn: 1280.0000 - val_accuracy: 0.8562 - val_precision: 0.1563 - val_recall: 0.6944 - val_auc: 0.8605 - val_prc: 0.4008\n",
            "Epoch 50/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.5018 - tp: 11151.0000 - fp: 75330.0000 - tn: 380628.0000 - fn: 5323.0000 - accuracy: 0.8293 - precision: 0.1289 - recall: 0.6769 - auc: 0.8324 - prc: 0.3168 - val_loss: 0.4733 - val_tp: 2984.0000 - val_fp: 17483.0000 - val_tn: 96436.0000 - val_fn: 1205.0000 - val_accuracy: 0.8418 - val_precision: 0.1458 - val_recall: 0.7123 - val_auc: 0.8597 - val_prc: 0.4060\n",
            "Epoch 51/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5012 - tp: 11161.0000 - fp: 75145.0000 - tn: 380813.0000 - fn: 5313.0000 - accuracy: 0.8297 - precision: 0.1293 - recall: 0.6775 - auc: 0.8330 - prc: 0.3123 - val_loss: 0.4591 - val_tp: 2824.0000 - val_fp: 14414.0000 - val_tn: 99505.0000 - val_fn: 1365.0000 - val_accuracy: 0.8664 - val_precision: 0.1638 - val_recall: 0.6741 - val_auc: 0.8595 - val_prc: 0.4012\n",
            "Epoch 52/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.5020 - tp: 11106.0000 - fp: 73678.0000 - tn: 382280.0000 - fn: 5368.0000 - accuracy: 0.8327 - precision: 0.1310 - recall: 0.6742 - auc: 0.8321 - prc: 0.3146 - val_loss: 0.4282 - val_tp: 2745.0000 - val_fp: 14746.0000 - val_tn: 99173.0000 - val_fn: 1444.0000 - val_accuracy: 0.8629 - val_precision: 0.1569 - val_recall: 0.6553 - val_auc: 0.8562 - val_prc: 0.3927\n",
            "Epoch 53/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5011 - tp: 11070.0000 - fp: 72858.0000 - tn: 383100.0000 - fn: 5404.0000 - accuracy: 0.8343 - precision: 0.1319 - recall: 0.6720 - auc: 0.8337 - prc: 0.3120 - val_loss: 0.4918 - val_tp: 3026.0000 - val_fp: 18290.0000 - val_tn: 95629.0000 - val_fn: 1163.0000 - val_accuracy: 0.8353 - val_precision: 0.1420 - val_recall: 0.7224 - val_auc: 0.8589 - val_prc: 0.3981\n",
            "Epoch 54/200\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.4995 - tp: 11156.0000 - fp: 71869.0000 - tn: 384089.0000 - fn: 5318.0000 - accuracy: 0.8366 - precision: 0.1344 - recall: 0.6772 - auc: 0.8344 - prc: 0.3193 - val_loss: 0.4728 - val_tp: 2997.0000 - val_fp: 17658.0000 - val_tn: 96261.0000 - val_fn: 1192.0000 - val_accuracy: 0.8404 - val_precision: 0.1451 - val_recall: 0.7154 - val_auc: 0.8603 - val_prc: 0.3968\n",
            "Epoch 55/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5048 - tp: 11037.0000 - fp: 75657.0000 - tn: 380301.0000 - fn: 5437.0000 - accuracy: 0.8283 - precision: 0.1273 - recall: 0.6700 - auc: 0.8301 - prc: 0.3115 - val_loss: 0.4710 - val_tp: 3056.0000 - val_fp: 19092.0000 - val_tn: 94827.0000 - val_fn: 1133.0000 - val_accuracy: 0.8288 - val_precision: 0.1380 - val_recall: 0.7295 - val_auc: 0.8590 - val_prc: 0.4000\n",
            "Epoch 56/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4987 - tp: 11203.0000 - fp: 75321.0000 - tn: 380637.0000 - fn: 5271.0000 - accuracy: 0.8294 - precision: 0.1295 - recall: 0.6800 - auc: 0.8352 - prc: 0.3214 - val_loss: 0.4686 - val_tp: 2910.0000 - val_fp: 15839.0000 - val_tn: 98080.0000 - val_fn: 1279.0000 - val_accuracy: 0.8551 - val_precision: 0.1552 - val_recall: 0.6947 - val_auc: 0.8606 - val_prc: 0.4080\n",
            "Epoch 57/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.5031 - tp: 11112.0000 - fp: 74070.0000 - tn: 381888.0000 - fn: 5362.0000 - accuracy: 0.8319 - precision: 0.1305 - recall: 0.6745 - auc: 0.8318 - prc: 0.3151 - val_loss: 0.4410 - val_tp: 2753.0000 - val_fp: 14078.0000 - val_tn: 99841.0000 - val_fn: 1436.0000 - val_accuracy: 0.8686 - val_precision: 0.1636 - val_recall: 0.6572 - val_auc: 0.8576 - val_prc: 0.4028\n",
            "Epoch 58/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5003 - tp: 11082.0000 - fp: 72332.0000 - tn: 383626.0000 - fn: 5392.0000 - accuracy: 0.8355 - precision: 0.1329 - recall: 0.6727 - auc: 0.8339 - prc: 0.3194 - val_loss: 0.4500 - val_tp: 2744.0000 - val_fp: 13783.0000 - val_tn: 100136.0000 - val_fn: 1445.0000 - val_accuracy: 0.8711 - val_precision: 0.1660 - val_recall: 0.6550 - val_auc: 0.8592 - val_prc: 0.4022\n",
            "Epoch 59/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4989 - tp: 11174.0000 - fp: 74040.0000 - tn: 381918.0000 - fn: 5300.0000 - accuracy: 0.8321 - precision: 0.1311 - recall: 0.6783 - auc: 0.8359 - prc: 0.3168 - val_loss: 0.4408 - val_tp: 2863.0000 - val_fp: 15046.0000 - val_tn: 98873.0000 - val_fn: 1326.0000 - val_accuracy: 0.8614 - val_precision: 0.1599 - val_recall: 0.6835 - val_auc: 0.8616 - val_prc: 0.4066\n",
            "Epoch 60/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5014 - tp: 10933.0000 - fp: 72727.0000 - tn: 383231.0000 - fn: 5541.0000 - accuracy: 0.8343 - precision: 0.1307 - recall: 0.6637 - auc: 0.8330 - prc: 0.3163 - val_loss: 0.5058 - val_tp: 3112.0000 - val_fp: 20718.0000 - val_tn: 93201.0000 - val_fn: 1077.0000 - val_accuracy: 0.8155 - val_precision: 0.1306 - val_recall: 0.7429 - val_auc: 0.8604 - val_prc: 0.3973\n",
            "Epoch 61/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5004 - tp: 10958.0000 - fp: 71472.0000 - tn: 384486.0000 - fn: 5516.0000 - accuracy: 0.8370 - precision: 0.1329 - recall: 0.6652 - auc: 0.8341 - prc: 0.3177 - val_loss: 0.4607 - val_tp: 2946.0000 - val_fp: 17304.0000 - val_tn: 96615.0000 - val_fn: 1243.0000 - val_accuracy: 0.8430 - val_precision: 0.1455 - val_recall: 0.7033 - val_auc: 0.8607 - val_prc: 0.4027\n",
            "Epoch 62/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.5002 - tp: 11017.0000 - fp: 71897.0000 - tn: 384061.0000 - fn: 5457.0000 - accuracy: 0.8363 - precision: 0.1329 - recall: 0.6688 - auc: 0.8339 - prc: 0.3185 - val_loss: 0.4977 - val_tp: 3120.0000 - val_fp: 20759.0000 - val_tn: 93160.0000 - val_fn: 1069.0000 - val_accuracy: 0.8152 - val_precision: 0.1307 - val_recall: 0.7448 - val_auc: 0.8610 - val_prc: 0.4073\n",
            "Epoch 63/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4979 - tp: 11137.0000 - fp: 71729.0000 - tn: 384229.0000 - fn: 5337.0000 - accuracy: 0.8369 - precision: 0.1344 - recall: 0.6760 - auc: 0.8353 - prc: 0.3248 - val_loss: 0.4415 - val_tp: 2921.0000 - val_fp: 15716.0000 - val_tn: 98203.0000 - val_fn: 1268.0000 - val_accuracy: 0.8562 - val_precision: 0.1567 - val_recall: 0.6973 - val_auc: 0.8637 - val_prc: 0.4104\n",
            "Epoch 64/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.5007 - tp: 11066.0000 - fp: 71593.0000 - tn: 384365.0000 - fn: 5408.0000 - accuracy: 0.8370 - precision: 0.1339 - recall: 0.6717 - auc: 0.8339 - prc: 0.3235 - val_loss: 0.4554 - val_tp: 2893.0000 - val_fp: 15593.0000 - val_tn: 98326.0000 - val_fn: 1296.0000 - val_accuracy: 0.8570 - val_precision: 0.1565 - val_recall: 0.6906 - val_auc: 0.8619 - val_prc: 0.4066\n",
            "Epoch 65/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4985 - tp: 11200.0000 - fp: 74008.0000 - tn: 381950.0000 - fn: 5274.0000 - accuracy: 0.8322 - precision: 0.1314 - recall: 0.6799 - auc: 0.8350 - prc: 0.3210 - val_loss: 0.4428 - val_tp: 2861.0000 - val_fp: 14806.0000 - val_tn: 99113.0000 - val_fn: 1328.0000 - val_accuracy: 0.8634 - val_precision: 0.1619 - val_recall: 0.6830 - val_auc: 0.8620 - val_prc: 0.4028\n",
            "Epoch 66/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.4985 - tp: 11094.0000 - fp: 70671.0000 - tn: 385287.0000 - fn: 5380.0000 - accuracy: 0.8390 - precision: 0.1357 - recall: 0.6734 - auc: 0.8340 - prc: 0.3259 - val_loss: 0.4573 - val_tp: 2996.0000 - val_fp: 17806.0000 - val_tn: 96113.0000 - val_fn: 1193.0000 - val_accuracy: 0.8391 - val_precision: 0.1440 - val_recall: 0.7152 - val_auc: 0.8610 - val_prc: 0.4084\n",
            "Epoch 67/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.5010 - tp: 11033.0000 - fp: 72809.0000 - tn: 383149.0000 - fn: 5441.0000 - accuracy: 0.8344 - precision: 0.1316 - recall: 0.6697 - auc: 0.8323 - prc: 0.3237 - val_loss: 0.4620 - val_tp: 2851.0000 - val_fp: 13944.0000 - val_tn: 99975.0000 - val_fn: 1338.0000 - val_accuracy: 0.8706 - val_precision: 0.1698 - val_recall: 0.6806 - val_auc: 0.8651 - val_prc: 0.4207\n",
            "Epoch 68/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.4980 - tp: 11016.0000 - fp: 69703.0000 - tn: 386255.0000 - fn: 5458.0000 - accuracy: 0.8409 - precision: 0.1365 - recall: 0.6687 - auc: 0.8342 - prc: 0.3278 - val_loss: 0.4278 - val_tp: 2723.0000 - val_fp: 11764.0000 - val_tn: 102155.0000 - val_fn: 1466.0000 - val_accuracy: 0.8880 - val_precision: 0.1880 - val_recall: 0.6500 - val_auc: 0.8642 - val_prc: 0.4161\n",
            "Epoch 69/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.5025 - tp: 10900.0000 - fp: 68866.0000 - tn: 387092.0000 - fn: 5574.0000 - accuracy: 0.8424 - precision: 0.1366 - recall: 0.6616 - auc: 0.8310 - prc: 0.3205 - val_loss: 0.4650 - val_tp: 3041.0000 - val_fp: 18435.0000 - val_tn: 95484.0000 - val_fn: 1148.0000 - val_accuracy: 0.8342 - val_precision: 0.1416 - val_recall: 0.7259 - val_auc: 0.8632 - val_prc: 0.4097\n",
            "Epoch 70/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4979 - tp: 11160.0000 - fp: 73131.0000 - tn: 382827.0000 - fn: 5314.0000 - accuracy: 0.8340 - precision: 0.1324 - recall: 0.6774 - auc: 0.8347 - prc: 0.3272 - val_loss: 0.4383 - val_tp: 2884.0000 - val_fp: 14182.0000 - val_tn: 99737.0000 - val_fn: 1305.0000 - val_accuracy: 0.8689 - val_precision: 0.1690 - val_recall: 0.6885 - val_auc: 0.8653 - val_prc: 0.4193\n",
            "Epoch 71/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4984 - tp: 11008.0000 - fp: 69208.0000 - tn: 386750.0000 - fn: 5466.0000 - accuracy: 0.8419 - precision: 0.1372 - recall: 0.6682 - auc: 0.8340 - prc: 0.3228 - val_loss: 0.4498 - val_tp: 2882.0000 - val_fp: 15312.0000 - val_tn: 98607.0000 - val_fn: 1307.0000 - val_accuracy: 0.8593 - val_precision: 0.1584 - val_recall: 0.6880 - val_auc: 0.8634 - val_prc: 0.4126\n",
            "Epoch 72/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4988 - tp: 11026.0000 - fp: 70625.0000 - tn: 385333.0000 - fn: 5448.0000 - accuracy: 0.8390 - precision: 0.1350 - recall: 0.6693 - auc: 0.8338 - prc: 0.3263 - val_loss: 0.4309 - val_tp: 2756.0000 - val_fp: 12755.0000 - val_tn: 101164.0000 - val_fn: 1433.0000 - val_accuracy: 0.8799 - val_precision: 0.1777 - val_recall: 0.6579 - val_auc: 0.8646 - val_prc: 0.4114\n",
            "Epoch 73/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4968 - tp: 11112.0000 - fp: 70931.0000 - tn: 385027.0000 - fn: 5362.0000 - accuracy: 0.8385 - precision: 0.1354 - recall: 0.6745 - auc: 0.8353 - prc: 0.3251 - val_loss: 0.4795 - val_tp: 2990.0000 - val_fp: 16975.0000 - val_tn: 96944.0000 - val_fn: 1199.0000 - val_accuracy: 0.8461 - val_precision: 0.1498 - val_recall: 0.7138 - val_auc: 0.8636 - val_prc: 0.4162\n",
            "Epoch 74/200\n",
            "116/116 [==============================] - 10s 83ms/step - loss: 0.4953 - tp: 11191.0000 - fp: 71948.0000 - tn: 384010.0000 - fn: 5283.0000 - accuracy: 0.8365 - precision: 0.1346 - recall: 0.6793 - auc: 0.8362 - prc: 0.3291 - val_loss: 0.4947 - val_tp: 2968.0000 - val_fp: 17446.0000 - val_tn: 96473.0000 - val_fn: 1221.0000 - val_accuracy: 0.8419 - val_precision: 0.1454 - val_recall: 0.7085 - val_auc: 0.8601 - val_prc: 0.4074\n",
            "Epoch 75/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.4977 - tp: 11111.0000 - fp: 71509.0000 - tn: 384449.0000 - fn: 5363.0000 - accuracy: 0.8373 - precision: 0.1345 - recall: 0.6745 - auc: 0.8349 - prc: 0.3312 - val_loss: 0.4576 - val_tp: 2859.0000 - val_fp: 14167.0000 - val_tn: 99752.0000 - val_fn: 1330.0000 - val_accuracy: 0.8688 - val_precision: 0.1679 - val_recall: 0.6825 - val_auc: 0.8652 - val_prc: 0.4228\n",
            "Epoch 76/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4972 - tp: 11137.0000 - fp: 73062.0000 - tn: 382896.0000 - fn: 5337.0000 - accuracy: 0.8341 - precision: 0.1323 - recall: 0.6760 - auc: 0.8355 - prc: 0.3297 - val_loss: 0.4578 - val_tp: 2734.0000 - val_fp: 12463.0000 - val_tn: 101456.0000 - val_fn: 1455.0000 - val_accuracy: 0.8822 - val_precision: 0.1799 - val_recall: 0.6527 - val_auc: 0.8615 - val_prc: 0.4136\n",
            "Epoch 77/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4950 - tp: 11167.0000 - fp: 71638.0000 - tn: 384320.0000 - fn: 5307.0000 - accuracy: 0.8371 - precision: 0.1349 - recall: 0.6779 - auc: 0.8362 - prc: 0.3313 - val_loss: 0.4611 - val_tp: 2978.0000 - val_fp: 18471.0000 - val_tn: 95448.0000 - val_fn: 1211.0000 - val_accuracy: 0.8334 - val_precision: 0.1388 - val_recall: 0.7109 - val_auc: 0.8605 - val_prc: 0.4039\n",
            "Epoch 78/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.4965 - tp: 11237.0000 - fp: 73197.0000 - tn: 382761.0000 - fn: 5237.0000 - accuracy: 0.8340 - precision: 0.1331 - recall: 0.6821 - auc: 0.8356 - prc: 0.3311 - val_loss: 0.4374 - val_tp: 2766.0000 - val_fp: 14154.0000 - val_tn: 99765.0000 - val_fn: 1423.0000 - val_accuracy: 0.8681 - val_precision: 0.1635 - val_recall: 0.6603 - val_auc: 0.8614 - val_prc: 0.4112\n",
            "Epoch 79/200\n",
            "116/116 [==============================] - 10s 84ms/step - loss: 0.4962 - tp: 11170.0000 - fp: 72653.0000 - tn: 383305.0000 - fn: 5304.0000 - accuracy: 0.8350 - precision: 0.1333 - recall: 0.6780 - auc: 0.8357 - prc: 0.3308 - val_loss: 0.4776 - val_tp: 2917.0000 - val_fp: 17934.0000 - val_tn: 95985.0000 - val_fn: 1272.0000 - val_accuracy: 0.8374 - val_precision: 0.1399 - val_recall: 0.6963 - val_auc: 0.8559 - val_prc: 0.3990\n",
            "Epoch 80/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4970 - tp: 11281.0000 - fp: 74084.0000 - tn: 381874.0000 - fn: 5193.0000 - accuracy: 0.8322 - precision: 0.1322 - recall: 0.6848 - auc: 0.8347 - prc: 0.3260 - val_loss: 0.4269 - val_tp: 2860.0000 - val_fp: 14467.0000 - val_tn: 99452.0000 - val_fn: 1329.0000 - val_accuracy: 0.8663 - val_precision: 0.1651 - val_recall: 0.6827 - val_auc: 0.8655 - val_prc: 0.4173\n",
            "Epoch 81/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4990 - tp: 11242.0000 - fp: 74922.0000 - tn: 381036.0000 - fn: 5232.0000 - accuracy: 0.8303 - precision: 0.1305 - recall: 0.6824 - auc: 0.8342 - prc: 0.3219 - val_loss: 0.4476 - val_tp: 2949.0000 - val_fp: 15534.0000 - val_tn: 98385.0000 - val_fn: 1240.0000 - val_accuracy: 0.8580 - val_precision: 0.1596 - val_recall: 0.7040 - val_auc: 0.8655 - val_prc: 0.4221\n",
            "Epoch 82/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4951 - tp: 11273.0000 - fp: 74234.0000 - tn: 381724.0000 - fn: 5201.0000 - accuracy: 0.8319 - precision: 0.1318 - recall: 0.6843 - auc: 0.8369 - prc: 0.3310 - val_loss: 0.4669 - val_tp: 3014.0000 - val_fp: 17387.0000 - val_tn: 96532.0000 - val_fn: 1175.0000 - val_accuracy: 0.8428 - val_precision: 0.1477 - val_recall: 0.7195 - val_auc: 0.8650 - val_prc: 0.4234\n",
            "Epoch 83/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4947 - tp: 11269.0000 - fp: 72653.0000 - tn: 383305.0000 - fn: 5205.0000 - accuracy: 0.8352 - precision: 0.1343 - recall: 0.6840 - auc: 0.8371 - prc: 0.3269 - val_loss: 0.4409 - val_tp: 2891.0000 - val_fp: 15770.0000 - val_tn: 98149.0000 - val_fn: 1298.0000 - val_accuracy: 0.8555 - val_precision: 0.1549 - val_recall: 0.6901 - val_auc: 0.8642 - val_prc: 0.4129\n",
            "Epoch 84/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4969 - tp: 11111.0000 - fp: 71011.0000 - tn: 384947.0000 - fn: 5363.0000 - accuracy: 0.8383 - precision: 0.1353 - recall: 0.6745 - auc: 0.8351 - prc: 0.3280 - val_loss: 0.4669 - val_tp: 2998.0000 - val_fp: 18271.0000 - val_tn: 95648.0000 - val_fn: 1191.0000 - val_accuracy: 0.8352 - val_precision: 0.1410 - val_recall: 0.7157 - val_auc: 0.8623 - val_prc: 0.4115\n",
            "Epoch 85/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4957 - tp: 11195.0000 - fp: 73460.0000 - tn: 382498.0000 - fn: 5279.0000 - accuracy: 0.8333 - precision: 0.1322 - recall: 0.6796 - auc: 0.8364 - prc: 0.3305 - val_loss: 0.4619 - val_tp: 2947.0000 - val_fp: 17117.0000 - val_tn: 96802.0000 - val_fn: 1242.0000 - val_accuracy: 0.8446 - val_precision: 0.1469 - val_recall: 0.7035 - val_auc: 0.8620 - val_prc: 0.4134\n",
            "Epoch 86/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4952 - tp: 11177.0000 - fp: 71638.0000 - tn: 384320.0000 - fn: 5297.0000 - accuracy: 0.8372 - precision: 0.1350 - recall: 0.6785 - auc: 0.8364 - prc: 0.3348 - val_loss: 0.4325 - val_tp: 2872.0000 - val_fp: 16062.0000 - val_tn: 97857.0000 - val_fn: 1317.0000 - val_accuracy: 0.8529 - val_precision: 0.1517 - val_recall: 0.6856 - val_auc: 0.8621 - val_prc: 0.4183\n",
            "Epoch 87/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4947 - tp: 11324.0000 - fp: 73176.0000 - tn: 382782.0000 - fn: 5150.0000 - accuracy: 0.8342 - precision: 0.1340 - recall: 0.6874 - auc: 0.8376 - prc: 0.3265 - val_loss: 0.4843 - val_tp: 3222.0000 - val_fp: 24469.0000 - val_tn: 89450.0000 - val_fn: 967.0000 - val_accuracy: 0.7846 - val_precision: 0.1164 - val_recall: 0.7692 - val_auc: 0.8602 - val_prc: 0.4134\n",
            "Epoch 88/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4956 - tp: 11049.0000 - fp: 71706.0000 - tn: 384252.0000 - fn: 5425.0000 - accuracy: 0.8367 - precision: 0.1335 - recall: 0.6707 - auc: 0.8362 - prc: 0.3337 - val_loss: 0.4214 - val_tp: 2846.0000 - val_fp: 13978.0000 - val_tn: 99941.0000 - val_fn: 1343.0000 - val_accuracy: 0.8703 - val_precision: 0.1692 - val_recall: 0.6794 - val_auc: 0.8657 - val_prc: 0.4270\n",
            "Epoch 89/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4931 - tp: 11189.0000 - fp: 71154.0000 - tn: 384804.0000 - fn: 5285.0000 - accuracy: 0.8382 - precision: 0.1359 - recall: 0.6792 - auc: 0.8384 - prc: 0.3350 - val_loss: 0.4542 - val_tp: 2920.0000 - val_fp: 16225.0000 - val_tn: 97694.0000 - val_fn: 1269.0000 - val_accuracy: 0.8519 - val_precision: 0.1525 - val_recall: 0.6971 - val_auc: 0.8645 - val_prc: 0.4171\n",
            "Epoch 90/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4947 - tp: 11335.0000 - fp: 75190.0000 - tn: 380768.0000 - fn: 5139.0000 - accuracy: 0.8300 - precision: 0.1310 - recall: 0.6881 - auc: 0.8376 - prc: 0.3299 - val_loss: 0.4667 - val_tp: 2924.0000 - val_fp: 16489.0000 - val_tn: 97430.0000 - val_fn: 1265.0000 - val_accuracy: 0.8497 - val_precision: 0.1506 - val_recall: 0.6980 - val_auc: 0.8617 - val_prc: 0.4166\n",
            "Epoch 91/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4935 - tp: 11365.0000 - fp: 75683.0000 - tn: 380275.0000 - fn: 5109.0000 - accuracy: 0.8290 - precision: 0.1306 - recall: 0.6899 - auc: 0.8381 - prc: 0.3374 - val_loss: 0.4497 - val_tp: 2974.0000 - val_fp: 17040.0000 - val_tn: 96879.0000 - val_fn: 1215.0000 - val_accuracy: 0.8454 - val_precision: 0.1486 - val_recall: 0.7100 - val_auc: 0.8648 - val_prc: 0.4245\n",
            "Epoch 92/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4946 - tp: 11269.0000 - fp: 74135.0000 - tn: 381823.0000 - fn: 5205.0000 - accuracy: 0.8321 - precision: 0.1319 - recall: 0.6840 - auc: 0.8362 - prc: 0.3353 - val_loss: 0.4570 - val_tp: 2720.0000 - val_fp: 11736.0000 - val_tn: 102183.0000 - val_fn: 1469.0000 - val_accuracy: 0.8882 - val_precision: 0.1882 - val_recall: 0.6493 - val_auc: 0.8643 - val_prc: 0.4261\n",
            "Epoch 93/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4928 - tp: 11311.0000 - fp: 73667.0000 - tn: 382291.0000 - fn: 5163.0000 - accuracy: 0.8331 - precision: 0.1331 - recall: 0.6866 - auc: 0.8383 - prc: 0.3366 - val_loss: 0.4303 - val_tp: 2947.0000 - val_fp: 17201.0000 - val_tn: 96718.0000 - val_fn: 1242.0000 - val_accuracy: 0.8438 - val_precision: 0.1463 - val_recall: 0.7035 - val_auc: 0.8619 - val_prc: 0.4086\n",
            "Epoch 94/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4947 - tp: 11329.0000 - fp: 73715.0000 - tn: 382243.0000 - fn: 5145.0000 - accuracy: 0.8331 - precision: 0.1332 - recall: 0.6877 - auc: 0.8364 - prc: 0.3328 - val_loss: 0.4246 - val_tp: 2847.0000 - val_fp: 14542.0000 - val_tn: 99377.0000 - val_fn: 1342.0000 - val_accuracy: 0.8655 - val_precision: 0.1637 - val_recall: 0.6796 - val_auc: 0.8663 - val_prc: 0.4306\n",
            "Epoch 95/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4957 - tp: 11484.0000 - fp: 78721.0000 - tn: 377237.0000 - fn: 4990.0000 - accuracy: 0.8228 - precision: 0.1273 - recall: 0.6971 - auc: 0.8369 - prc: 0.3313 - val_loss: 0.4732 - val_tp: 2985.0000 - val_fp: 17398.0000 - val_tn: 96521.0000 - val_fn: 1204.0000 - val_accuracy: 0.8425 - val_precision: 0.1464 - val_recall: 0.7126 - val_auc: 0.8646 - val_prc: 0.4248\n",
            "Epoch 96/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4923 - tp: 11318.0000 - fp: 74881.0000 - tn: 381077.0000 - fn: 5156.0000 - accuracy: 0.8306 - precision: 0.1313 - recall: 0.6870 - auc: 0.8379 - prc: 0.3394 - val_loss: 0.4505 - val_tp: 2832.0000 - val_fp: 15712.0000 - val_tn: 98207.0000 - val_fn: 1357.0000 - val_accuracy: 0.8555 - val_precision: 0.1527 - val_recall: 0.6761 - val_auc: 0.8612 - val_prc: 0.4147\n",
            "Epoch 97/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4917 - tp: 11250.0000 - fp: 73534.0000 - tn: 382424.0000 - fn: 5224.0000 - accuracy: 0.8333 - precision: 0.1327 - recall: 0.6829 - auc: 0.8379 - prc: 0.3397 - val_loss: 0.4196 - val_tp: 2871.0000 - val_fp: 14071.0000 - val_tn: 99848.0000 - val_fn: 1318.0000 - val_accuracy: 0.8697 - val_precision: 0.1695 - val_recall: 0.6854 - val_auc: 0.8651 - val_prc: 0.4315\n",
            "Epoch 98/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4985 - tp: 11159.0000 - fp: 75032.0000 - tn: 380926.0000 - fn: 5315.0000 - accuracy: 0.8299 - precision: 0.1295 - recall: 0.6774 - auc: 0.8341 - prc: 0.3313 - val_loss: 0.4778 - val_tp: 3062.0000 - val_fp: 18238.0000 - val_tn: 95681.0000 - val_fn: 1127.0000 - val_accuracy: 0.8360 - val_precision: 0.1438 - val_recall: 0.7310 - val_auc: 0.8660 - val_prc: 0.4239\n",
            "Epoch 99/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4937 - tp: 11307.0000 - fp: 73622.0000 - tn: 382336.0000 - fn: 5167.0000 - accuracy: 0.8332 - precision: 0.1331 - recall: 0.6864 - auc: 0.8371 - prc: 0.3376 - val_loss: 0.4315 - val_tp: 2831.0000 - val_fp: 15325.0000 - val_tn: 98594.0000 - val_fn: 1358.0000 - val_accuracy: 0.8587 - val_precision: 0.1559 - val_recall: 0.6758 - val_auc: 0.8615 - val_prc: 0.4158\n",
            "Epoch 100/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4931 - tp: 11451.0000 - fp: 75283.0000 - tn: 380675.0000 - fn: 5023.0000 - accuracy: 0.8300 - precision: 0.1320 - recall: 0.6951 - auc: 0.8379 - prc: 0.3359 - val_loss: 0.4404 - val_tp: 2974.0000 - val_fp: 16499.0000 - val_tn: 97420.0000 - val_fn: 1215.0000 - val_accuracy: 0.8500 - val_precision: 0.1527 - val_recall: 0.7100 - val_auc: 0.8679 - val_prc: 0.4269\n",
            "Epoch 101/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4922 - tp: 11266.0000 - fp: 72080.0000 - tn: 383878.0000 - fn: 5208.0000 - accuracy: 0.8364 - precision: 0.1352 - recall: 0.6839 - auc: 0.8390 - prc: 0.3366 - val_loss: 0.4698 - val_tp: 3019.0000 - val_fp: 18585.0000 - val_tn: 95334.0000 - val_fn: 1170.0000 - val_accuracy: 0.8327 - val_precision: 0.1397 - val_recall: 0.7207 - val_auc: 0.8633 - val_prc: 0.4211\n",
            "Epoch 102/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4935 - tp: 11397.0000 - fp: 75455.0000 - tn: 380503.0000 - fn: 5077.0000 - accuracy: 0.8295 - precision: 0.1312 - recall: 0.6918 - auc: 0.8374 - prc: 0.3375 - val_loss: 0.4570 - val_tp: 3194.0000 - val_fp: 22118.0000 - val_tn: 91801.0000 - val_fn: 995.0000 - val_accuracy: 0.8043 - val_precision: 0.1262 - val_recall: 0.7625 - val_auc: 0.8655 - val_prc: 0.4256\n",
            "Epoch 103/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4904 - tp: 11351.0000 - fp: 73682.0000 - tn: 382276.0000 - fn: 5123.0000 - accuracy: 0.8332 - precision: 0.1335 - recall: 0.6890 - auc: 0.8402 - prc: 0.3414 - val_loss: 0.4433 - val_tp: 2880.0000 - val_fp: 14746.0000 - val_tn: 99173.0000 - val_fn: 1309.0000 - val_accuracy: 0.8641 - val_precision: 0.1634 - val_recall: 0.6875 - val_auc: 0.8667 - val_prc: 0.4310\n",
            "Epoch 104/200\n",
            "116/116 [==============================] - 10s 85ms/step - loss: 0.4937 - tp: 11249.0000 - fp: 71697.0000 - tn: 384261.0000 - fn: 5225.0000 - accuracy: 0.8372 - precision: 0.1356 - recall: 0.6828 - auc: 0.8377 - prc: 0.3360 - val_loss: 0.4423 - val_tp: 2906.0000 - val_fp: 16071.0000 - val_tn: 97848.0000 - val_fn: 1283.0000 - val_accuracy: 0.8531 - val_precision: 0.1531 - val_recall: 0.6937 - val_auc: 0.8645 - val_prc: 0.4143\n",
            "Epoch 105/200\n",
            "116/116 [==============================] - 10s 86ms/step - loss: 0.4925 - tp: 11314.0000 - fp: 72788.0000 - tn: 383170.0000 - fn: 5160.0000 - accuracy: 0.8350 - precision: 0.1345 - recall: 0.6868 - auc: 0.8375 - prc: 0.3353 - val_loss: 0.4507 - val_tp: 2944.0000 - val_fp: 15814.0000 - val_tn: 98105.0000 - val_fn: 1245.0000 - val_accuracy: 0.8556 - val_precision: 0.1569 - val_recall: 0.7028 - val_auc: 0.8652 - val_prc: 0.4268\n",
            "Epoch 106/200\n",
            "116/116 [==============================] - 10s 87ms/step - loss: 0.4925 - tp: 11433.0000 - fp: 75798.0000 - tn: 380160.0000 - fn: 5041.0000 - accuracy: 0.8289 - precision: 0.1311 - recall: 0.6940 - auc: 0.8375 - prc: 0.3368 - val_loss: 0.4620 - val_tp: 3010.0000 - val_fp: 18731.0000 - val_tn: 95188.0000 - val_fn: 1179.0000 - val_accuracy: 0.8314 - val_precision: 0.1384 - val_recall: 0.7185 - val_auc: 0.8622 - val_prc: 0.4241\n",
            "Epoch 107/200\n",
            "115/116 [============================>.] - ETA: 0s - loss: 0.4923 - tp: 11390.0000 - fp: 75957.0000 - tn: 378662.0000 - fn: 5031.0000 - accuracy: 0.8281 - precision: 0.1304 - recall: 0.6936 - auc: 0.8384 - prc: 0.3388Restoring model weights from the end of the best epoch: 97.\n",
            "116/116 [==============================] - 10s 88ms/step - loss: 0.4923 - tp: 11429.0000 - fp: 76230.0000 - tn: 379728.0000 - fn: 5045.0000 - accuracy: 0.8280 - precision: 0.1304 - recall: 0.6938 - auc: 0.8384 - prc: 0.3389 - val_loss: 0.4575 - val_tp: 3001.0000 - val_fp: 16756.0000 - val_tn: 97163.0000 - val_fn: 1188.0000 - val_accuracy: 0.8481 - val_precision: 0.1519 - val_recall: 0.7164 - val_auc: 0.8672 - val_prc: 0.4295\n",
            "Epoch 107: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_actual, y_pred):\n",
        "    #Get the confusion matrix, Precision & Re-call\n",
        "    conf_matrix = confusion_matrix(y_actual, y_pred)\n",
        "    recal_matrix =(((conf_matrix.T)/(conf_matrix.sum(axis=1))).T)        \n",
        "    prec_matrix =(conf_matrix/conf_matrix.sum(axis=0))\n",
        "\n",
        "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                    conf_matrix.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                        conf_matrix.flatten()/np.sum(conf_matrix)]\n",
        "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
        "              zip(group_names,group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "\n",
        "    plt.figure(figsize=(20,4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.heatmap(conf_matrix, annot=labels, fmt='', cmap='Blues')\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.title(\"Confusion matrix\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.heatmap(prec_matrix, annot=True, fmt='.3f', cmap='Blues')\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.title(\"Precision matrix\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.heatmap(recal_matrix, annot=True, fmt='.3f', cmap='Blues')\n",
        "    plt.xlabel('Predicted Class')\n",
        "    plt.ylabel('Original Class')\n",
        "    plt.title(\"Recall matrix\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xHMMqmIJpRJN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "val_predictions_weighted = weighted_model.predict(val_features, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "PuX2fo4lZ7mb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_results = weighted_model.evaluate(val_features, val_labels,\n",
        "                                           batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "print('Test confusion_matrix')\n",
        "plot_confusion_matrix(val_labels, val_predictions_weighted >0.5)"
      ],
      "metadata": {
        "id": "hqwI1Q5WdXNT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "c0d5e6bf-9a7e-4916-fb8e-edb9f2fbcfa0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss :  0.4195968508720398\n",
            "tp :  2871.0\n",
            "fp :  14071.0\n",
            "tn :  99848.0\n",
            "fn :  1318.0\n",
            "accuracy :  0.869704008102417\n",
            "precision :  0.1694605052471161\n",
            "recall :  0.6853664517402649\n",
            "auc :  0.8651365041732788\n",
            "prc :  0.4315303564071655\n",
            "\n",
            "Test confusion_matrix\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = weighted_model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "submission = {}\n",
        "submission.update(dict(zip(test_data['TransactionID_x'],y_pred_test)))\n",
        "submission = pd.DataFrame.from_dict(submission, orient=\"index\").reset_index()\n",
        "submission.columns = [\"TransactionID\", \"isFraud\"]\n",
        "submission.to_csv('/gdrive/My Drive/AIML/Project/submission3.csv',index=False)\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "Jldt2Mt6qw95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1d3edf23-58bc-4dbc-8f21-d13ca7ed2869"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-12efb474-0d28-4914-b08a-c4e00011d52e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TransactionID</th>\n",
              "      <th>isFraud</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3663549</td>\n",
              "      <td>0.115361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3663550</td>\n",
              "      <td>0.328533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3663551</td>\n",
              "      <td>0.462967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3663552</td>\n",
              "      <td>0.150531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3663553</td>\n",
              "      <td>0.128603</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12efb474-0d28-4914-b08a-c4e00011d52e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-12efb474-0d28-4914-b08a-c4e00011d52e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-12efb474-0d28-4914-b08a-c4e00011d52e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   TransactionID   isFraud\n",
              "0        3663549  0.115361\n",
              "1        3663550  0.328533\n",
              "2        3663551  0.462967\n",
              "3        3663552  0.150531\n",
              "4        3663553  0.128603"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}